# syntax=docker/dockerfile:1.2
ARG MERLIN_VERSION=22.12
ARG TRITON_VERSION=22.11
ARG TORCH_VERSION=22.11

ARG DLFW_IMAGE=nvcr.io/nvidia/pytorch:${TORCH_VERSION}-py3
ARG FULL_IMAGE=nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-py3
ARG BASE_IMAGE=nvcr.io/nvstaging/merlin/merlin-base:${MERLIN_VERSION}

FROM ${DLFW_IMAGE} as dlfw
FROM ${FULL_IMAGE} as triton
FROM ${BASE_IMAGE} as base
# Install packages
RUN apt update -y --fix-missing && \
    apt install -y --no-install-recommends \
        libatlas-base-dev \
        libmkl-dev \
    apt autoremove -y && \
    apt clean && \
    rm -rf /var/lib/apt/lists/*

# Torch Metrics (without torch)
RUN pip install --no-cache-dir --no-deps torch torchmetrics \
        && pip install --upgrade pip && python -m pip cache purge \
        && rm -rf /usr/local/lib/python3.8/dist-packages/torch \
        && rm -rf /usr/local/lib/python3.8/dist-packages/caffe2

# Triton Torch backend
COPY --chown=1000:1000 --from=triton /opt/tritonserver/backends/pytorch backends/pytorch

# DLFW Python packages
COPY --chown=1000:1000 --from=dlfw /usr/local/lib/python3.8/dist-packages/numba /usr/local/lib/python3.8/dist-packages/numba
COPY --chown=1000:1000 --from=dlfw /usr/local/lib/python3.8/dist-packages/numpy /usr/local/lib/python3.8/dist-packages/numpy
COPY --chown=1000:1000 --from=dlfw /usr/local/lib/python3.8/dist-packages/torch /usr/local/lib/python3.8/dist-packages/torch

COPY --chown=1000:1000 --from=dlfw /usr/local/lib/python3.8/dist-packages/numba-*.dist-info /usr/local/lib/python3.8/dist-packages/numba.dist-info/
COPY --chown=1000:1000 --from=dlfw /usr/local/lib/python3.8/dist-packages/numpy-*.dist-info /usr/local/lib/python3.8/dist-packages/numpy.dist-info/
COPY --chown=1000:1000 --from=dlfw /usr/local/lib/python3.8/dist-packages/torch-*.egg-info /usr/local/lib/python3.8/dist-packages/torch.egg-info/


# Add all torch libraries to /usr/local
RUN ln -s /opt/tritonserver/backends/pytorch/* /usr/local/lib/

RUN pip install matplotlib

# Install HPS Backend
ARG HUGECTR_DEV_MODE=false
ARG HUGECTR_VER=main
ARG _HUGECTR_REPO="github.com/NVIDIA-Merlin/HugeCTR.git"
ARG HUGECTR_BACKEND_VER=main
ARG _CI_JOB_TOKEN=""
ARG _HUGECTR_BACKEND_REPO="github.com/triton-inference-server/hugectr_backend.git"
ARG HUGECTR_HOME=/usr/local/hugectr
ARG TRITON_VERSION

ENV PATH=$PATH:${HUGECTR_HOME}/bin \
    CPATH=$CPATH:${HUGECTR_HOME}/include \
    LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HUGECTR_HOME}/lib

RUN if [ "${HUGECTR_DEV_MODE}" == "false" ]; then \
        # Install HugeCTR inference which is dependency for hps_backenc
        git clone --branch ${HUGECTR_VER} --depth 1 https://${_CI_JOB_TOKEN}${_HUGECTR_REPO} /hugectr && \
        cd /hugectr && \
        git submodule update --init --recursive && \
        mkdir build && \
        cd build && \
        cmake -DCMAKE_BUILD_TYPE=Release -DSM="60;61;70;75;80" -DENABLE_INFERENCE=ON .. && \
        make -j$(nproc) && \
        make install && \
        cd / && rm -rf /hugectr && \
        # Install hps_backend
        git clone --branch ${HUGECTR_BACKEND_VER} --depth 1 https://${_CI_JOB_TOKEN}${_HUGECTR_BACKEND_REPO} /repos/hugectr_triton_backend && \
        mkdir /repos/hugectr_triton_backend/hps_backend/build && \
        cd /repos/hugectr_triton_backend/hps_backend/build && \
        cmake \
            -DCMAKE_INSTALL_PREFIX:PATH=${HUGECTR_HOME} \
            -DTRITON_COMMON_REPO_TAG="r${TRITON_VERSION}" \
            -DTRITON_CORE_REPO_TAG="r${TRITON_VERSION}" \
            -DTRITON_BACKEND_REPO_TAG="r${TRITON_VERSION}" .. && \
        make -j$(nproc) && \
        make install && \
        cd ../.. && \
        rm -rf hugectr_triton_backend && \
        chmod +x ${HUGECTR_HOME}/lib/*.so ${HUGECTR_HOME}/backends/hps/*.so \
    ; fi
RUN ln -s ${HUGECTR_HOME}/backends/hps /opt/tritonserver/backends/hps
