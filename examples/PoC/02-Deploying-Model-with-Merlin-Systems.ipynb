{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "## Deploying the Model into Production with Merlin Systems and Triton IS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9657308-2e08-49b4-8924-eace75a4634c",
   "metadata": {},
   "source": [
    "At this point, when you reach out to this notebook, we expect that you have already executed the first notebook `01-Building-Recommender-Systems-PoC.ipynb` and exported all the required files and models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvtabular/nvtabular/graph.py:23: FutureWarning: The `nvtabular.graph` module has moved to `merlin.dag`. Support for importing from `nvtabular.graph` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.dag`.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/io.py:23: FutureWarning: The `nvtabular.io` module has moved to `merlin.io`. Support for importing from `nvtabular.io` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.io`.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/utils.py:23: FutureWarning: The `nvtabular.utils` module has moved to `merlin.core.utils`. Support for importing from `nvtabular.utils` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.core.utils`.\n",
      "  warnings.warn(\n",
      "/nvtabular/nvtabular/dispatch.py:23: FutureWarning: The `nvtabular.dispatch` module has moved to `merlin.core.dispatch`. Support for importing from `nvtabular.dispatch` is deprecated, and will be removed in a future version. Please update your imports to import from `merlin.core.dispatch`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "\n",
    "from merlin.systems.triton.utils import run_triton_server, run_ensemble_on_tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ead20e-c573-462e-9aa2-c3494bf0129f",
   "metadata": {},
   "source": [
    "### Feast Apply and Materialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac115e-4794-4a69-a962-8481f6e86df3",
   "metadata": {},
   "source": [
    "We have defined our user and item features definitions in the `user_features.py` and  `item_features.py` files. With FeatureView() users can register data sources in their organizations into Feast, and then use those data sources for both training and online inference. In the `user_features.py` and `item_features.py` files, we are telling Feast where to find user and item features.\n",
    "\n",
    "\n",
    "Before we move on to the next steps, we need to perform `apply` and `materizalize` commands as directed below:\n",
    "\n",
    "```\n",
    "# open a terminal and navigate to the `feature_repo` folder\n",
    "\n",
    "cd /Merlin/examples/PoC/feature_repo\n",
    "\n",
    "# run the following commands\n",
    "\n",
    "feast apply\n",
    "feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01\n",
    "```\n",
    "\n",
    "With `feast apply` we can apply the changes to create our feature registry and store all entity and feature view definitions in a local file called `registry.db`.\n",
    "\n",
    "When you run the `feast materialize ..` command you will see a print out message <i>Materializing 2 feature views from 1995-01-01 01:01:01+00:00 to 2025-01-01 01:01:01+00:00 into the sqlite online store </i> on your terminal. [materialization](https://docs.feast.dev/how-to-guides/running-feast-in-production) operation is done to keep our online store up to date. For that we need to run a job that loads feature data from our feature view sources into our online store. As we add new features to our offline stores, we can continuously materialize them to keep our online store fresh. \n",
    "\n",
    "Note that materialization step takes some time.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc26e6-f6f3-4e44-bf3c-3b8e66dc9fd6",
   "metadata": {},
   "source": [
    "Now, let's check our feature_repo structure again after we ran `apply` and `materialize` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caba4e3-e6e0-4e2f-b51d-cd3456fd4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./feature_repo\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── item_features.parquet\n",
      "│   ├── online_store.db\n",
      "│   ├── registry.db\n",
      "│   └── user_features.parquet\n",
      "├── feature_store.yaml\n",
      "├── item_features.py\n",
      "└── user_features.py\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./feature_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa0ef91-dd66-46e5-8f51-470d277429b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/systems/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f3194-9f17-4fa7-8baa-14333f2a122a",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- \n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b267f2-1d27-476c-b0cc-5564b429b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvtabular.loader.tf_utils import configure_tensorflow, get_dataset_schema_from_feature_columns\n",
    "\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b7adc1-623b-41df-b1f9-dd4086a15bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Merlin/examples/PoC/\"\n",
    "faiss_index_path = base_path + 'tmp' + \"/index.faiss\"\n",
    "feast_repo_path = base_path + \"feature_repo/\"\n",
    "retrieval_model_path = base_path + \"query_tower/\"\n",
    "ranking_model_path = base_path + \"dlrm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e5f98-45aa-4421-984e-1659d3fc6a69",
   "metadata": {},
   "source": [
    "To help you visualize what we will be doing, here’s what our feature_repo structure look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7103033-1860-4857-ab6e-99b9581b24ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Merlin/examples/PoC/tmp/index.faiss',\n",
       " '/Merlin/examples/PoC/feature_repo/',\n",
       " '/Merlin/examples/PoC/query_tower/')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_index_path, feast_repo_path, retrieval_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eac635d-f029-480e-9b0c-d1d9b26c6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
      "/usr/local/lib/python3.8/dist-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "03/30/2022 12:40:14 AM INFO:Loading faiss with AVX2 support.\n",
      "03/30/2022 12:40:14 AM INFO:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "03/30/2022 12:40:14 AM INFO:Loading faiss.\n",
      "03/30/2022 12:40:14 AM INFO:Successfully loaded faiss.\n",
      "/systems/merlin/systems/dag/ops/feast.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ValueType.FLOAT: (np.float, False, False),\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cudf\n",
    "import feast\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss \n",
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "\n",
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")\n",
    "\n",
    "item_embeddings = np.ascontiguousarray(\n",
    "    pd.read_parquet(base_path + \"item_embeddings.parquet\").to_numpy()\n",
    ")\n",
    "\n",
    "feature_store = feast.FeatureStore(feast_repo_path)\n",
    "setup_faiss(item_embeddings, faiss_index_path)\n",
    "\n",
    "user_features = [\"user_id\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    path=feast_repo_path,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c2d9b1-51dc-4549-977d-d7941ee6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 00:40:16.934151: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 00:40:18.123955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 00:40:20.257596: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1034311152 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2022 12:40:21 AM WARNING:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "retrieval = (\n",
    "    user_features\n",
    "    >> PredictTensorflow(retrieval_model_path)\n",
    "    >> QueryFaiss(faiss_index_path, topk=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b270f663-0ae1-4356-acd4-5f8c986abf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    path=feast_repo_path,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")\n",
    "\n",
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce31723e-af4d-4827-bb60-3a9fafcd9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 00:40:25.126858: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n",
      "2022-03-30 00:40:25.127006: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)\n",
    "\n",
    "ordering = combined_features[\"item_id\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"output_1\"], topk=10, temperature=20.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff84601-41fc-4e30-bd6a-f9a7d927e980",
   "metadata": {},
   "source": [
    "The last step of machine learning (ML)/deep learning (DL) pipeline is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as done during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the DL model for a prediction. Therefore, we deploy the NVTabular workflow with the Tensorflow model as an ensemble model to Triton Inference using Merlin Systems library very easily. The ensemble model guarantees that the same transformation is applied to the raw inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c64d686-aed5-42f8-b517-482b4237c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "export_path = '/Merlin/examples/PoC/poc_ensemble/'\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)\n",
    "\n",
    "request = make_df({\"user_id\": [1]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7896ec0-db89-4642-bfb6-eebf9afe77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 00:40:28.996325 1892 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "I0330 00:40:28.996416 1892 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:28.996421 1892 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:28.996425 1892 tensorflow.cc:2216] backend configuration:\n",
      "{\"cmdline\":{\"version\":\"2\"}}\n",
      "I0330 00:40:29.150263 1892 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f96b4000000' with size 268435456\n",
      "I0330 00:40:29.150648 1892 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0330 00:40:29.156027 1892 model_repository_manager.cc:994] loading: 0_queryfeast:1\n",
      "I0330 00:40:29.256479 1892 model_repository_manager.cc:994] loading: 1_predicttensorflow:1\n",
      "I0330 00:40:29.263985 1892 backend.cc:46] TRITONBACKEND_Initialize: nvtabular\n",
      "I0330 00:40:29.264019 1892 backend.cc:53] Triton TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:29.264031 1892 backend.cc:56] 'nvtabular' TRITONBACKEND API version: 1.8\n",
      "I0330 00:40:29.264435 1892 backend.cc:76] Loaded libpython successfully\n",
      "I0330 00:40:29.356963 1892 model_repository_manager.cc:994] loading: 2_queryfaiss:1\n",
      "I0330 00:40:29.457427 1892 model_repository_manager.cc:994] loading: 3_queryfeast:1\n",
      "I0330 00:40:29.498327 1892 backend.cc:89] Python interpreter is initialized\n",
      "I0330 00:40:29.500531 1892 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 1_predicttensorflow (version 1)\n",
      "I0330 00:40:29.502226 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/0_queryfeast/1'\n",
      "I0330 00:40:29.557917 1892 model_repository_manager.cc:994] loading: 4_unrollfeatures:1\n",
      "I0330 00:40:29.658398 1892 model_repository_manager.cc:994] loading: 5_predicttensorflow:1\n",
      "I0330 00:40:29.758632 1892 model_repository_manager.cc:994] loading: 6_softmaxsampling:1\n",
      "I0330 00:40:31.637752 1892 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 1_predicttensorflow (GPU device 0)\n",
      "I0330 00:40:31.637888 1892 model_repository_manager.cc:1149] successfully loaded '0_queryfeast' version 1\n",
      "2022-03-30 00:40:32.773008: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/PoC/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:32.777114: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-03-30 00:40:32.777150: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/PoC/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:32.780174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13586 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 00:40:32.833536: W tensorflow/core/common_runtime/colocation_graph.cc:1218] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n",
      "  /job:localhost/replica:0/task:0/device:CPU:0].\n",
      "See below for details of this colocation group:\n",
      "Colocation Debug Info:\n",
      "Colocation group had the following types and supported devices: \n",
      "Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n",
      "ReadVariableOp: GPU CPU \n",
      "VarHandleOp: CPU \n",
      "\n",
      "Colocation members, user-requested devices, and framework assigned devices, if any:\n",
      "  retrieval_model/sequential_block_20/item_id (VarHandleOp) /gpu:0\n",
      "  retrieval_model/sequential_block_20/item_id/Read/ReadVariableOp (ReadVariableOp) /gpu:0\n",
      "\n",
      "2022-03-30 00:40:32.833643: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-03-30 00:40:33.842139: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/PoC/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:33.872860: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 1099867 microseconds.\n",
      "I0330 00:40:33.873051 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/2_queryfaiss/1'\n",
      "I0330 00:40:33.873177 1892 model_repository_manager.cc:1149] successfully loaded '1_predicttensorflow' version 1\n",
      "/systems/merlin/systems/dag/ops/feast.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ValueType.FLOAT: (np.float, False, False),\n",
      "/usr/local/lib/python3.8/dist-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
      "/usr/local/lib/python3.8/dist-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "03/30/2022 12:40:33 AM INFO:Loading faiss with AVX2 support.\n",
      "03/30/2022 12:40:33 AM INFO:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "03/30/2022 12:40:33 AM INFO:Loading faiss.\n",
      "03/30/2022 12:40:33 AM INFO:Successfully loaded faiss.\n",
      "I0330 00:40:34.504436 1892 model_repository_manager.cc:1149] successfully loaded '2_queryfaiss' version 1\n",
      "I0330 00:40:34.510268 1892 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 5_predicttensorflow (version 1)\n",
      "I0330 00:40:34.512085 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/3_queryfeast/1'\n",
      "I0330 00:40:34.528076 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/4_unrollfeatures/1'\n",
      "I0330 00:40:34.528133 1892 model_repository_manager.cc:1149] successfully loaded '3_queryfeast' version 1\n",
      "I0330 00:40:34.532301 1892 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 5_predicttensorflow (GPU device 0)\n",
      "I0330 00:40:34.532432 1892 model_repository_manager.cc:1149] successfully loaded '4_unrollfeatures' version 1\n",
      "2022-03-30 00:40:34.532746: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/PoC/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:34.549321: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-03-30 00:40:34.549362: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/PoC/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:34.551203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13586 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 00:40:34.575881: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-03-30 00:40:35.632967: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/PoC/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 00:40:35.689019: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 1156284 microseconds.\n",
      "I0330 00:40:35.689217 1892 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/PoC/poc_ensemble/6_softmaxsampling/1'\n",
      "I0330 00:40:35.689308 1892 model_repository_manager.cc:1149] successfully loaded '5_predicttensorflow' version 1\n",
      "I0330 00:40:35.690666 1892 model_repository_manager.cc:1149] successfully loaded '6_softmaxsampling' version 1\n",
      "I0330 00:40:35.695317 1892 model_repository_manager.cc:994] loading: ensemble_model:1\n",
      "I0330 00:40:35.796054 1892 model_repository_manager.cc:1149] successfully loaded 'ensemble_model' version 1\n",
      "I0330 00:40:35.796301 1892 server.cc:522] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0330 00:40:35.796415 1892 server.cc:549] \n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| Backend    | Path                                                            | Config                      |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| tensorflow | /opt/tritonserver/backends/tensorflow2/libtriton_tensorflow2.so | {\"cmdline\":{\"version\":\"2\"}} |\n",
      "| nvtabular  | /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so     | {}                          |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "\n",
      "I0330 00:40:35.796560 1892 server.cc:592] \n",
      "+---------------------+---------+--------+\n",
      "| Model               | Version | Status |\n",
      "+---------------------+---------+--------+\n",
      "| 0_queryfeast        | 1       | READY  |\n",
      "| 1_predicttensorflow | 1       | READY  |\n",
      "| 2_queryfaiss        | 1       | READY  |\n",
      "| 3_queryfeast        | 1       | READY  |\n",
      "| 4_unrollfeatures    | 1       | READY  |\n",
      "| 5_predicttensorflow | 1       | READY  |\n",
      "| 6_softmaxsampling   | 1       | READY  |\n",
      "| ensemble_model      | 1       | READY  |\n",
      "+---------------------+---------+--------+\n",
      "\n",
      "I0330 00:40:35.870830 1892 metrics.cc:623] Collecting metrics for GPU 0: Quadro GV100\n",
      "I0330 00:40:35.871672 1892 tritonserver.cc:1932] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                        |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                       |\n",
      "| server_version                   | 2.19.0                                                                                                                                                                                       |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "| model_repository_path[0]         | /Merlin/examples/PoC/poc_ensemble/                                                                                                                                                           |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0330 00:40:35.883657 1892 grpc_server.cc:4375] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0330 00:40:35.885572 1892 http_server.cc:3075] Started HTTPService at 0.0.0.0:8000\n",
      "I0330 00:40:35.928305 1892 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal (2) received.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 00:40:44.842471 1892 server.cc:252] Waiting for in-flight requests to complete.\n",
      "I0330 00:40:44.842502 1892 model_repository_manager.cc:1026] unloading: ensemble_model:1\n",
      "I0330 00:40:44.842628 1892 model_repository_manager.cc:1026] unloading: 6_softmaxsampling:1\n",
      "I0330 00:40:44.842775 1892 model_repository_manager.cc:1026] unloading: 5_predicttensorflow:1\n",
      "I0330 00:40:44.842846 1892 model_repository_manager.cc:1132] successfully unloaded 'ensemble_model' version 1\n",
      "I0330 00:40:44.842869 1892 model_repository_manager.cc:1026] unloading: 4_unrollfeatures:1\n",
      "I0330 00:40:44.843011 1892 model_repository_manager.cc:1026] unloading: 3_queryfeast:1\n",
      "I0330 00:40:44.843074 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0330 00:40:44.843092 1892 model_repository_manager.cc:1026] unloading: 2_queryfaiss:1\n",
      "Signal (I0330 00:40:44.843153 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0330 00:40:44.843172 1892 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0330 00:40:44.843189 1892 model_repository_manager.cc:1026] unloading: 1_predicttensorflow:111\n",
      ") received.\n",
      "\n",
      "I0330 00:40:44.843277 1892 model_repository_manager.cc:1026] unloading: 0_queryfeast:1\n",
      "Signal (11) received.\n",
      "I0330 00:40:44.843299 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.I0330 00:40:44.843368 1892 server.cc:267] Timeout 30: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:44.843487 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "Signal (11) received.\n",
      "I0330 00:40:44.843547 1892 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.\n",
      "I0330 00:40:44.843600 1892 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model stateI0330 00:40:44.843606 1892 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "I0330 00:40:44.843731 1892 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0330 00:40:45.853598 1892 server.cc:267] Timeout 29: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:46.942861 1892 server.cc:267] Timeout 28: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:47.972121 1892 server.cc:267] Timeout 27: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:49.004065 1892 server.cc:267] Timeout 26: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 00:40:50.037075 1892 server.cc:267] Timeout 25: Found 7 live models and 0 in-flight non-inference requests\n",
      " 0# 0x0000564A8941E299 in /opt/tritonserver/bin/tritonserver\n",
      " 1# 0x00007F9748ED3210 in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      " 2# 0x00007F96F28BDF2E in /usr/lib/x86_64-linux-gnu/libpython3.8.so.1.0\n",
      " 3# TRITONBACKEND_ModelInstanceFinalize in /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so\n",
      " 4# 0x00007F9749A70FC4 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 5# 0x00007F9749A6A3B9 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 6# 0x00007F9749A6AB1D in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 7# 0x00007F97498EE0D7 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 8# 0x00007F97492C1DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
      " 9# 0x00007F974973F609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\n",
      "10# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = run_ensemble_on_tritonserver(\n",
    "    export_path, ensemble.graph.output_schema.column_names, request, \"ensemble_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840b471-8153-4d5f-82f8-f77614470ca4",
   "metadata": {},
   "source": [
    "Convert our response to a numpy array and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "583e6354-183a-4dae-8533-bfc643d4452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2628383],\n",
       "       [1780891],\n",
       "       [ 397955],\n",
       "       [2573239],\n",
       "       [1255680],\n",
       "       [ 505277],\n",
       "       [2084603],\n",
       "       [ 365618],\n",
       "       [ 229051],\n",
       "       [1323574]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output= response.as_numpy('ordered_ids')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "Note that these item ids are encoded values, not the raw original values. We will eventually create the reverse dictionary lookup functionality to be able to map these encoded item ids to their original raw ids with one-line of code. But if you really want to do it now, you can easily map these ids to their original values using the `unique.item_id.parquet` file stored in the `categories` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6445f-7fc0-40e1-8d9a-21749c8acd1e",
   "metadata": {},
   "source": [
    "Well done! You finished deploying a 4-stage Recommender Systems on Triton Inference Server using Merlin framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
