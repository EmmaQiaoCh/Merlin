{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "## Deploying the Model into Production with Merlin Systems and Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9657308-2e08-49b4-8924-eace75a4634c",
   "metadata": {},
   "source": [
    "At this point, when you reach out to this notebook, we expect that you have already executed the first notebook `01-Building-Recommender-Systems-PoC.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![tritonensemble](../images/triton_ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "Merlin Systems library have the set of operators to be able to serve multi-stage recommender systems built with Tensorflow on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently. Below, we will go through these operators and demonstrate their usage in serving a multi-stage system on Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import feast\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import run_triton_server, run_ensemble_on_tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ead20e-c573-462e-9aa2-c3494bf0129f",
   "metadata": {},
   "source": [
    "### Register our features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac115e-4794-4a69-a962-8481f6e86df3",
   "metadata": {},
   "source": [
    "We have defined our user and item features definitions in the `user_features.py` and  `item_features.py` files. With FeatureView() users can register data sources in their organizations into Feast, and then use those data sources for both training and online inference. In the `user_features.py` and `item_features.py` files, we are telling Feast where to find user and item features.\n",
    "\n",
    "Before we move on to the next steps, we need to perform `feast apply`command as directed below.  With that, we register our features, we can apply the changes to create our feature registry and store all entity and feature view definitions in a local SQLite online store called `online_store.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fa545b-a979-4216-b176-ffd70d66e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/feast/feature_view.py:100: DeprecationWarning: The argument 'input' is being deprecated. Please use 'batch_source' instead. Feast 0.13 and onwards will not support the argument 'input'.\n",
      "  warnings.warn(\n",
      "Created data source \u001b[1m\u001b[32m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/data/item_features.parquet\u001b[0m\n",
      "Created data source \u001b[1m\u001b[32m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo/data/user_features.parquet\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32muser_id\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mitem_id\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mitem_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32muser_features\u001b[0m\n",
      "\n",
      "Created sqlite table \u001b[1m\u001b[32mfeature_repo_item_features\u001b[0m\n",
      "Created sqlite table \u001b[1m\u001b[32mfeature_repo_user_features\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd /Merlin/examples/Deploying-multi-stage-RecSys/feature_repo\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641fcd2-bd11-4569-80d4-2ae5e01a5cad",
   "metadata": {},
   "source": [
    "### Feast Materialize\n",
    "\n",
    "After we execute `apply` and registered our features and created our online local store, now we need to perform [materialization](https://docs.feast.dev/how-to-guides/running-feast-in-production) operation. This is done to keep our online store up to date and get it ready for prediction. For that we need to run a job that loads feature data from our feature view sources into our online store. As we add new features to our offline stores, we can continuously materialize them to keep our online store up to date by finding the latest feature values for each user. \n",
    "\n",
    "When you run the `feast materialize ..` command below, you will see a message <i>Materializing 2 feature views from 1995-01-01 01:01:01+00:00 to 2025-01-01 01:01:01+00:00 into the sqlite online store </i>  printed out below\n",
    "\n",
    "Note that materialization step takes some time.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dacbbc-bdb6-4f7a-b202-3802050f0362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m1995-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m:\n",
      "100%|███████████████████████████████████████████████████| 3078306/3078306 [14:53<00:00, 3447.06it/s]\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m:\n",
      "100%|█████████████████████████████████████████████████████| 294736/294736 [03:18<00:00, 1488.17it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc26e6-f6f3-4e44-bf3c-3b8e66dc9fd6",
   "metadata": {},
   "source": [
    "Now, let's check our feature_repo structure again after we ran `apply` and `materialize` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9caba4e3-e6e0-4e2f-b51d-cd3456fd4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/Merlin/examples/Deploying-multi-stage-RecSys/feature_repo\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── item_features.parquet\n",
      "│   ├── online_store.db\n",
      "│   ├── registry.db\n",
      "│   └── user_features.parquet\n",
      "├── feature_store.yaml\n",
      "├── item_features.py\n",
      "└── user_features.py\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "# set up the base dir to for feature store\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/Merlin/examples/Deploying-multi-stage-RecSys/\")\n",
    "feature_repo_path = os.path.join(BASE_DIR, 'feature_repo')\n",
    "!tree $feature_repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6b18a-40e0-4e4a-823e-19020c068d89",
   "metadata": {},
   "source": [
    "We use `configure_tensorflow` function to prevent the Tensorflow to claim entire GPU memory. With this func, we let TF to allocate 50% of the available GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b267f2-1d27-476c-b0cc-5564b429b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2022 10:48:55 PM INFO:init\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efada1e1-2556-4a26-b0ba-9cb96b3b151f",
   "metadata": {},
   "source": [
    "Create a folder for faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b7adc1-623b-41df-b1f9-dd4086a15bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR + 'faiss_index')):\n",
    "    os.makedirs(os.path.join(BASE_DIR + 'faiss_index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking model, retrieval model, feast feature repo and faiss index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = BASE_DIR + 'faiss_index' + \"/index.faiss\"\n",
    "feast_repo_path = BASE_DIR + \"feature_repo/\"\n",
    "retrieval_model_path = BASE_DIR + \"query_tower/\"\n",
    "ranking_model_path = BASE_DIR + \"dlrm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b530f15-81c9-4c81-8962-c86ee0247245",
   "metadata": {},
   "source": [
    "Create a request schema that we are going to use when sending a request to Triton Infrence Server (TIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cdda540-8209-49f9-8b6a-4b330570fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffb994-384c-436e-b49e-0ea5589d8116",
   "metadata": {},
   "source": [
    "### Set up Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd5a53-79f4-40e1-baef-80d710ac2cef",
   "metadata": {},
   "source": [
    "`QueryFaiss` operator creates an interface between a FAISS Approximate Nearest Neighbors (ANN) Index and Triton Infrence Server. For a given input query vector, we do an ANN search query to find the ids of top-k nearby nodes in the index. \n",
    "\n",
    "`QueryFeast` operator is responsible for ensuring that our feast feature store can communicate correctly with tritonserver for the ensemble feast feature look ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec702617-d2fd-4d5b-8a00-8069e0f31274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss \n",
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "item_embeddings = np.ascontiguousarray(\n",
    "    pd.read_parquet(BASE_DIR + \"item_embeddings.parquet\").to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b996019-bd2a-44e0-b004-4f412b300d63",
   "metadata": {},
   "source": [
    "`setup_faiss` is  a utility function that will create a Faiss index from an embedding vector with using L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b6cc5bf-d07c-4963-a748-6e2b4827ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = feast.FeatureStore(feast_repo_path)\n",
    "setup_faiss(item_embeddings, faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45df06-0cbe-4b52-ac1f-786e763895d7",
   "metadata": {},
   "source": [
    "Fetch user features with `QueryFeast` operator from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3decbe7b-03e3-4978-baac-03f6a0b078c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = [\"user_id\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    path=feast_repo_path,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e25be7-3ff0-49c2-a3fc-03ec4d615e77",
   "metadata": {},
   "source": [
    "Retrieve top-K candidate items using `retrieval model` that are relevant for a given user. We use `PredictTensorflow()` operator that takes a tensorflow model and packages it correctly for TIS to run with the tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c2d9b1-51dc-4549-977d-d7941ee6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 22:55:32.999743: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 22:55:34.169618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2022 10:55:36 PM WARNING:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "retrieval = (\n",
    "    user_features\n",
    "    >> PredictTensorflow(retrieval_model_path)\n",
    "    >> QueryFaiss(faiss_index_path, topk=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4429c-1fe1-4304-bcdf-badebe3b5485",
   "metadata": {},
   "source": [
    "Fetch item features for the candidate items that are retrieved from the retrieval step above from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b270f663-0ae1-4356-acd4-5f8c986abf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    path=feast_repo_path,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a4d09-db05-4666-b520-75dbbbc7ab17",
   "metadata": {},
   "source": [
    "Merge the user features and items features to create the all set of combined features that were used in model training using `UnrollFeatures` operator which takes a target column and joins the \"unroll\" columns to the target. This helps when broadcasting a series of user features to a set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb0ef434-03a5-4a36-afb9-e19a43243c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0ce66-6b6c-43be-885e-a5435c3bbd9e",
   "metadata": {},
   "source": [
    "Rank the combined features using the trained ranking model, which is a DLRM model for this example. We feed the path of the ranking model to `PredictTensorflow()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce31723e-af4d-4827-bb60-3a9fafcd9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86fa47-de61-4007-ab55-9076e12ce963",
   "metadata": {},
   "source": [
    "For the ordering we use `SoftmaxSampling()` operator. This operator sorts all inputs in descending order given the input ids and prediction introducing some randomization into the ordering by sampling items from the softmax of the predicted relevance scores, and finally returns top-k ordered items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f65598b-e3e7-4238-a73e-19d00c3deb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = combined_features[\"item_id\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"output_1\"], topk=10, temperature=20.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2e389-d884-44a1-8e32-4916a0eb43cf",
   "metadata": {},
   "source": [
    "### Export Graph as Ensemble\n",
    "The last step is to create the ensemble artifacts that TIS can consume. To make these artifacts import the Ensemble class. This class  represents an entire ensemble consisting of multiple models that run sequentially in TIS initiated by an inference request. It is responsible with interpreting the graph and exporting the correct files for TIS.\n",
    "\n",
    "When we create an Ensemble object we feed the graph and a schema representing the starting input of the graph.  After we create the ensemble object, we export the graph, supplying an export path for the `ensemble.export()` function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc2e4f-5e58-4ad4-8ae5-d79ad286978f",
   "metadata": {},
   "source": [
    "Create the folder to export the models and config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b28c452f-543c-45a4-9995-130ca6919669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR + 'poc_ensemble')):\n",
    "    os.makedirs(os.path.join(BASE_DIR + 'poc_ensemble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c64d686-aed5-42f8-b517-482b4237c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path where all the models and config files exported to\n",
    "export_path = os.path.join(BASE_DIR + 'poc_ensemble')\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276eedd8-5dc0-4ad0-8725-c8da60fea693",
   "metadata": {},
   "source": [
    "Let's check our export_path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3c81b2a-4fca-497b-8edf-5403fe5a483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble\u001b[00m\n",
      "├── \u001b[01;34m0_queryfeast\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m1_predicttensorflow\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│   │       ├── \u001b[01;34massets\u001b[00m\n",
      "│   │       ├── keras_metadata.pb\n",
      "│   │       ├── saved_model.pb\n",
      "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
      "│   │           ├── variables.data-00000-of-00001\n",
      "│   │           └── variables.index\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m2_queryfaiss\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   ├── \u001b[01;34mindex.faiss\u001b[00m\n",
      "│   │   │   └── index.faiss\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m3_queryfeast\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m4_unrollfeatures\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m5_predicttensorflow\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "│   │       ├── \u001b[01;34massets\u001b[00m\n",
      "│   │       ├── keras_metadata.pb\n",
      "│   │       ├── saved_model.pb\n",
      "│   │       └── \u001b[01;34mvariables\u001b[00m\n",
      "│   │           ├── variables.data-00000-of-00001\n",
      "│   │           └── variables.index\n",
      "│   └── config.pbtxt\n",
      "├── \u001b[01;34m6_softmaxsampling\u001b[00m\n",
      "│   ├── \u001b[01;34m1\u001b[00m\n",
      "│   │   └── model.py\n",
      "│   └── config.pbtxt\n",
      "└── \u001b[01;34mensemble_model\u001b[00m\n",
      "    ├── \u001b[01;34m1\u001b[00m\n",
      "    └── config.pbtxt\n",
      "\n",
      "23 directories, 22 files\n"
     ]
    }
   ],
   "source": [
    "!tree $export_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fe264-e4a4-4dab-9b04-f83fb696d7d1",
   "metadata": {},
   "source": [
    "It is time to deploy the all the models as an ensemble model to Triton Inference very easily using Merlin Systems library. Now we can launch our triton server and load our models, and get a response for our query with a utility function `run_ensemble_on_tritonserver()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7896ec0-db89-4642-bfb6-eebf9afe77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 22:57:40.613079 11012 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "I0330 22:57:40.613268 11012 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.8\n",
      "I0330 22:57:40.613283 11012 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.8\n",
      "I0330 22:57:40.613299 11012 tensorflow.cc:2216] backend configuration:\n",
      "{\"cmdline\":{\"version\":\"2\"}}\n",
      "I0330 22:57:40.775601 11012 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7faec4000000' with size 268435456\n",
      "I0330 22:57:40.775981 11012 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0330 22:57:40.781158 11012 model_repository_manager.cc:994] loading: 0_queryfeast:1\n",
      "I0330 22:57:40.881596 11012 model_repository_manager.cc:994] loading: 1_predicttensorflow:1\n",
      "I0330 22:57:40.889764 11012 backend.cc:46] TRITONBACKEND_Initialize: nvtabular\n",
      "I0330 22:57:40.889792 11012 backend.cc:53] Triton TRITONBACKEND API version: 1.8\n",
      "I0330 22:57:40.889805 11012 backend.cc:56] 'nvtabular' TRITONBACKEND API version: 1.8\n",
      "I0330 22:57:40.890109 11012 backend.cc:76] Loaded libpython successfully\n",
      "I0330 22:57:40.981856 11012 model_repository_manager.cc:994] loading: 2_queryfaiss:1\n",
      "I0330 22:57:41.062648 11012 backend.cc:89] Python interpreter is initialized\n",
      "I0330 22:57:41.063839 11012 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 1_predicttensorflow (version 1)\n",
      "I0330 22:57:41.068091 11012 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/0_queryfeast/1'\n",
      "I0330 22:57:41.082119 11012 model_repository_manager.cc:994] loading: 3_queryfeast:1\n",
      "I0330 22:57:41.185350 11012 model_repository_manager.cc:994] loading: 4_unrollfeatures:1\n",
      "I0330 22:57:41.285844 11012 model_repository_manager.cc:994] loading: 5_predicttensorflow:1\n",
      "I0330 22:57:41.386218 11012 model_repository_manager.cc:994] loading: 6_softmaxsampling:1\n",
      "I0330 22:57:43.101469 11012 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 1_predicttensorflow (GPU device 0)\n",
      "I0330 22:57:43.101640 11012 model_repository_manager.cc:1149] successfully loaded '0_queryfeast' version 1\n",
      "2022-03-30 22:57:46.485295: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 22:57:46.497478: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-03-30 22:57:46.497583: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 22:57:46.511995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13586 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 22:57:46.606230: W tensorflow/core/common_runtime/colocation_graph.cc:1218] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\n",
      "  /job:localhost/replica:0/task:0/device:CPU:0].\n",
      "See below for details of this colocation group:\n",
      "Colocation Debug Info:\n",
      "Colocation group had the following types and supported devices: \n",
      "Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n",
      "ReadVariableOp: GPU CPU \n",
      "VarHandleOp: CPU \n",
      "\n",
      "Colocation members, user-requested devices, and framework assigned devices, if any:\n",
      "  retrieval_model/sequential_block_20/item_id (VarHandleOp) /gpu:0\n",
      "  retrieval_model/sequential_block_20/item_id/Read/ReadVariableOp (ReadVariableOp) /gpu:0\n",
      "\n",
      "2022-03-30 22:57:46.607550: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-03-30 22:57:47.624165: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/1_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 22:57:47.667021: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 1181753 microseconds.\n",
      "I0330 22:57:47.667436 11012 model_repository_manager.cc:1149] successfully loaded '1_predicttensorflow' version 1\n",
      "I0330 22:57:47.669477 11012 tensorflow.cc:2276] TRITONBACKEND_ModelInitialize: 5_predicttensorflow (version 1)\n",
      "I0330 22:57:47.672537 11012 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/2_queryfaiss/1'\n",
      "/systems/merlin/systems/dag/ops/feast.py:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ValueType.FLOAT: (np.float, False, False),\n",
      "/usr/local/lib/python3.8/dist-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
      "/usr/local/lib/python3.8/dist-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "03/30/2022 10:57:47 PM INFO:Loading faiss with AVX2 support.\n",
      "03/30/2022 10:57:47 PM INFO:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "03/30/2022 10:57:47 PM INFO:Loading faiss.\n",
      "03/30/2022 10:57:47 PM INFO:Successfully loaded faiss.\n",
      "I0330 22:57:48.151244 11012 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/3_queryfeast/1'\n",
      "I0330 22:57:48.151375 11012 model_repository_manager.cc:1149] successfully loaded '2_queryfaiss' version 1\n",
      "I0330 22:57:48.160289 11012 model_repository_manager.cc:1149] successfully loaded '3_queryfeast' version 1\n",
      "I0330 22:57:48.165111 11012 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/4_unrollfeatures/1'\n",
      "I0330 22:57:48.167043 11012 tensorflow.cc:2325] TRITONBACKEND_ModelInstanceInitialize: 5_predicttensorflow (GPU device 0)\n",
      "I0330 22:57:48.167256 11012 model_repository_manager.cc:1149] successfully loaded '4_unrollfeatures' version 1\n",
      "2022-03-30 22:57:48.167417: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 22:57:48.178834: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-03-30 22:57:48.178874: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 22:57:48.180387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13586 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-03-30 22:57:48.202645: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\n",
      "2022-03-30 22:57:48.991500: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/5_predicttensorflow/1/model.savedmodel\n",
      "2022-03-30 22:57:49.046639: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 879232 microseconds.\n",
      "I0330 22:57:49.046792 11012 model_inst_state.hpp:64] Loading TritonPythonnModel from model.py in path '/Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble/6_softmaxsampling/1'\n",
      "I0330 22:57:49.046878 11012 model_repository_manager.cc:1149] successfully loaded '5_predicttensorflow' version 1\n",
      "I0330 22:57:49.048397 11012 model_repository_manager.cc:1149] successfully loaded '6_softmaxsampling' version 1\n",
      "I0330 22:57:49.052742 11012 model_repository_manager.cc:994] loading: ensemble_model:1\n",
      "I0330 22:57:49.153492 11012 model_repository_manager.cc:1149] successfully loaded 'ensemble_model' version 1\n",
      "I0330 22:57:49.153725 11012 server.cc:522] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0330 22:57:49.153862 11012 server.cc:549] \n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| Backend    | Path                                                            | Config                      |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "| tensorflow | /opt/tritonserver/backends/tensorflow2/libtriton_tensorflow2.so | {\"cmdline\":{\"version\":\"2\"}} |\n",
      "| nvtabular  | /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so     | {}                          |\n",
      "+------------+-----------------------------------------------------------------+-----------------------------+\n",
      "\n",
      "I0330 22:57:49.154062 11012 server.cc:592] \n",
      "+---------------------+---------+--------+\n",
      "| Model               | Version | Status |\n",
      "+---------------------+---------+--------+\n",
      "| 0_queryfeast        | 1       | READY  |\n",
      "| 1_predicttensorflow | 1       | READY  |\n",
      "| 2_queryfaiss        | 1       | READY  |\n",
      "| 3_queryfeast        | 1       | READY  |\n",
      "| 4_unrollfeatures    | 1       | READY  |\n",
      "| 5_predicttensorflow | 1       | READY  |\n",
      "| 6_softmaxsampling   | 1       | READY  |\n",
      "| ensemble_model      | 1       | READY  |\n",
      "+---------------------+---------+--------+\n",
      "\n",
      "I0330 22:57:49.210864 11012 metrics.cc:623] Collecting metrics for GPU 0: Quadro GV100\n",
      "I0330 22:57:49.211267 11012 tritonserver.cc:1932] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                        |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                       |\n",
      "| server_version                   | 2.19.0                                                                                                                                                                                       |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "| model_repository_path[0]         | /Merlin/examples/Deploying-multi-stage-RecSys/poc_ensemble                                                                                                                                   |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0330 22:57:49.219226 11012 grpc_server.cc:4375] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0330 22:57:49.221039 11012 http_server.cc:3075] Started HTTPService at 0.0.0.0:8000\n",
      "I0330 22:57:49.263499 11012 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal (2) received.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 22:57:55.632500 11012 server.cc:252] Waiting for in-flight requests to complete.\n",
      "I0330 22:57:55.632531 11012 model_repository_manager.cc:1026] unloading: ensemble_model:1\n",
      "I0330 22:57:55.632647 11012 model_repository_manager.cc:1026] unloading: 6_softmaxsampling:1\n",
      "I0330 22:57:55.632770 11012 model_repository_manager.cc:1026] unloading: 5_predicttensorflow:1\n",
      "I0330 22:57:55.632872 11012 model_repository_manager.cc:1132] successfully unloaded 'ensemble_model' version 1\n",
      "I0330 22:57:55.632954 11012 model_repository_manager.cc:1026] unloading: 4_unrollfeatures:1\n",
      "I0330 22:57:55.633005 11012 model_repository_manager.cc:1026] unloading: 3_queryfeast:1\n",
      "I0330 22:57:55.633000 11012 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "Signal (11) received.\n",
      "I0330 22:57:55.633048 11012 model_repository_manager.cc:1026] unloading: 2_queryfaiss:1\n",
      "I0330 22:57:55.633214 11012 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0330 22:57:55.633231 11012 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0330 22:57:55.633237 11012 model_repository_manager.cc:1026] unloading: 1_predicttensorflow:1\n",
      "\n",
      "I0330 22:57:55.633276 11012 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "\n",
      "Signal (11) received.Signal (11) received.\n",
      "\n",
      "I0330 22:57:55.633399 11012 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0330 22:57:55.633416 11012 model_repository_manager.cc:1026] unloading: 0_queryfeast:1\n",
      "Signal (11) received.\n",
      "I0330 22:57:55.633423 11012 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0330 22:57:55.633491 11012 server.cc:267] Timeout 30: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 22:57:55.633582 11012 tensorflow.cc:2363] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0330 22:57:55.633725 11012 tensorflow.cc:2302] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0330 22:57:55.633738 11012 backend.cc:160] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "Signal (11) received.\n",
      "I0330 22:57:56.662061 11012 server.cc:267] Timeout 29: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 22:57:57.691743 11012 server.cc:267] Timeout 28: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 22:57:58.716561 11012 server.cc:267] Timeout 27: Found 7 live models and 0 in-flight non-inference requests\n",
      "I0330 22:57:59.733247 11012 server.cc:267] Timeout 26: Found 7 live models and 0 in-flight non-inference requests\n",
      " 0# 0x0000558C4351E299 in /opt/tritonserver/bin/tritonserver\n",
      " 1# 0x00007FAF56B46210 in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      " 2# 0x00007FAF00530F2E in /usr/lib/x86_64-linux-gnu/libpython3.8.so.1.0\n",
      " 3# TRITONBACKEND_ModelInstanceFinalize in /opt/tritonserver/backends/nvtabular/libtriton_nvtabular.so\n",
      " 4# 0x00007FAF576E3FC4 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 5# 0x00007FAF576DD3B9 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 6# 0x00007FAF576DDB1D in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 7# 0x00007FAF575610D7 in /opt/tritonserver/bin/../lib/libtritonserver.so\n",
      " 8# 0x00007FAF56F34DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
      " 9# 0x00007FAF573B2609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\n",
      "10# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a request to be sent to TIS\n",
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "request = make_df({\"user_id\": [1]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)\n",
    "\n",
    "response = run_ensemble_on_tritonserver(\n",
    "    export_path, ensemble.graph.output_schema.column_names, request, \"ensemble_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840b471-8153-4d5f-82f8-f77614470ca4",
   "metadata": {},
   "source": [
    "Convert our response to a numpy array and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "583e6354-183a-4dae-8533-bfc643d4452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1418158],\n",
       "       [ 861350],\n",
       "       [1562428],\n",
       "       [1979510],\n",
       "       [ 795631],\n",
       "       [1339132],\n",
       "       [ 729802],\n",
       "       [1168971],\n",
       "       [2988914],\n",
       "       [1357524]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output= response.as_numpy('ordered_ids')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "Note that these item ids are encoded values, not the raw original values. We will eventually create the reverse dictionary lookup functionality to be able to map these encoded item ids to their original raw ids with one-line of code. But if you really want to do it now, you can easily map these ids to their original values using the `unique.item_id.parquet` file stored in the `categories` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6445f-7fc0-40e1-8d9a-21749c8acd1e",
   "metadata": {},
   "source": [
    "That's it! You finished deploying a multi-stage Recommender Systems on Triton Inference Server using Merlin framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
